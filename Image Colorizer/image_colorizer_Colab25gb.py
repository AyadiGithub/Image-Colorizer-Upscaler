# -*- coding: utf-8 -*-
"""Image Colorizer 25GB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HClc7Nx5ygE0TpbMSDfHbBN9O_Fi07u3
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# In[1]: Imports
import os
import re
from skimage.transform import resize, rescale
from skimage.color import rgb2lab, lab2rgb, rgb2gray
from skimage.io import imsave
import matplotlib.pyplot as plt
import numpy as np
np.random.seed(0)
print("Tensorflow version " + tf.__version__)

import tensorflow.keras as keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.python.client import device_lib
from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, BatchNormalization, UpSampling2D
from tensorflow.keras.layers import Conv2DTranspose, UpSampling2D, add
from tensorflow.keras.models import Model
from tensorflow.keras import regularizers
from tensorflow.keras import backend
from PIL import Image
device_lib.list_local_devices()

# Download and Unzip File
!wget https://ucaa60aee564c4f8f73864a72d87.dl.dropboxusercontent.com/cd/0/get/A7WmqH2HUmDej03LGZMIsGyDGwKDWA6T0Yn8WTA-PlHJTSjlYlZ6CTUWCVNI2FJoLuvXS6TpLa_SOwgPb_pZaKM_zUQS_dX6qoo44VBzPTMI1xptS0n_DOGHrgeb0bXiFOQ/file

!unzip '/content/file'

# Encoder for the Autoencoder

# Input layer will be 256x256x1 (Grayscale LAB Image)
input_img = Input(shape = (256, 256, 1)) # Using Input class from Keras

layer1 = Conv2D(64, (3, 3), padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(input_img)  # Applied to the output of previous layer      

layerm1 = MaxPooling2D(padding='same')(layer1)  

layer2 = Conv2D(64, (3, 3), padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(layerm1)  # Applied to the output of previous layer 

layer3 = Conv2D(128, (3, 3), padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(layer2)  # Applied to the output of previous layer              

layerm2 = MaxPooling2D(padding='same')(layer3) 

layer4 = Conv2D(128, (3, 3), padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(layerm2)   # Applied to the output of previous layer 

layer5 = Conv2D(256, (3, 3), padding = 'same',
                activation = 'relu', padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(layer4)  # Applied to the output of previous layer 

layerm3 = MaxPooling2D(padding='same')(layer5) 

layer6 = Conv2D(512, (3, 3), padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(layerm3)  # Applied to the output of previous layer 

layer7 = Conv2D(512, (3, 3), padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(layer6)  # Applied to the output of previous layer 

# Model class from keras to put the model together
encoder = Model(input_img, layer7)

# Encoder model summary
encoder.summary()

# Reverse the Encoder for the Decoder
layer8 = Conv2DTranspose(128, (3, 3), 
                         strides=(2, 2), 
                         padding='same',
                         activation='relu', 
                         kernel_initializer='he_normal')(layer7)  # Applied to the output of previous layer 

layeradd = add([layer8, layer4])

layer9 = Conv2D(128, (3, 3), padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(layeradd)  # Applied to the output of previous layer 

layer10 = Conv2DTranspose(64, (3, 3), 
                         strides=(2, 2), 
                         padding='same',
                         activation='relu', 
                         kernel_initializer='he_normal')(layer9)  # Applied to the output of previous layer 

layer11 = Conv2DTranspose(64, (3, 3), padding='same',
                activation='relu', 
                kernel_initializer='he_normal')(layer10)  # Applied to the output of previous layer 

layeradd1 = add([layer11, layer2 ])

layer12 = Conv2DTranspose(32, (3, 3), 
                         strides=(2, 2), 
                         padding='same',
                         activation='relu', 
                         kernel_initializer='he_normal')(layeradd1)  # Applied to the output of previous layer 

# Decoder output, 2 channels for colored LAB Image
decoder = Conv2D(2, (3, 3), padding='same',
               activation='tanh', 
               kernel_initializer='glorot_normal')(layer12)  # Applied to the output of previous layer 

# combine the encoder and decoder to an Autoencoder using Model class
autoencoder = Model(input_img, decoder)

# Autoencoder summary
autoencoder.summary()

# Custom Loss function DSSIMLOSS to try out

def dssimloss(y_true, y_pred):
    ssim = tf.image.ssim(y_true, y_pred, 2.0)

    return backend.mean((1 - ssim)/2)

# Load model if already trained
#autoencoder = tf.keras.models.load_model('/content/Colorizer.h5')
#autoencoder.summary()

# Define optimizer and compile Model Autoencoder
autoencoder.compile(optimizer='adam', loss=dssimloss, metrics=['accuracy'])

# Import google drive to colab
from google.colab import drive
drive.mount('/content/drive')

# Checkpoint to save best weights

checkpoint_filepath = '/content/checkpoints/'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

# Reducing learning rate if no decrease in Val_loss - Non adaptive optimizer 
# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,
                              #patience=50, min_lr=1e-5)

# Train function


def train_batches(just_load_dataset=False):

    # Training on Tesla P-100 (16GB VRAM) 
    # Batch size
    batches = 128
    # point in current batch
    batch = 0

    # Current batch number
    batch_nb = 0

    max_batches = -1 #No limit to number of batches

    # number of epochs
    epochs = 50

    x_train_orig = []
    y_train_orig = []

    x_train = []  
    y_train = []  

    data_set_path = '/content/DatasetV1'

    for dirpath, dirnames, filenames in os.walk(data_set_path):

        for filename in filenames:
            if re.search("\.(jpg|jpeg|JPEG|png)$", filename):
                if batch_nb == max_batches:
                    return x_train, y_train

                filepath = os.path.join(dirpath, filename)  # set path for each image

                image = plt.imread(filepath)  # read image from path

                if len(image.shape) > 2:

                  lab = rgb2lab(image)  # Convert image from RGB to LAB
                  image_resized = resize(lab, (256, 256))  # Resizing images
                  x_train_orig.append(image_resized[:,:,0])  
                  y_train_orig.append(image_resized[:,:,1:] / 128)  
                  batch += 1

                  if batch == batches:

                      batch_nb += 1

                      y_train = np.array(y_train_orig)  # Convert to numpy array
                      x_train = np.array(x_train_orig)  # Convert to numpy array
                      x_train = x_train.reshape(x_train.shape+(1,))  # Reshape

                      if just_load_dataset:  # If just_load_dataset is set to True
                          return x_train , y_train  # return arrays

                      print('Training batch', batch_nb, '(', batches, ')')

                      autoencoder.fit(x_train, y_train, epochs=epochs, batch_size=64, validation_split=0.20, shuffle=True, callbacks=[model_checkpoint_callback])

                      x_train_orig = []
                      y_train_orig = []

                      batch = 0

    return x_train, y_train

x_train_orig, y_train_orig = train_batches()

autoencoder.save('/content/ColorizerSSIM.h5')

# Saving model to json
autoencoder_json = autoencoder.to_json()
with open("model.json", "w") as json_file:  # Store model.json
    json_file.write(autoencoder_json)

autoencoder.save_weights('/content/autoencoder_weightsv3.h5')
encoder.save_weights('/content/encoder_weightsv3.h5')

autoencoder.load_weights('/content/autoencoder_weightsv3.h5')
encoder.load_weights('/content/encoder_weightsv3.h5')

# Test images to evaluate the model
img1_color = []
img1 = img_to_array(load_img('/content/ali-liston.jpg'))

img1 = resize(img1 ,(256,256))
img1_color.append(img1)

img1_color = np.array(img1_color, dtype=float)
img1_color = rgb2lab(1.0/255*img1_color)[:,:,:,0]
img1_color = img1_color.reshape(img1_color.shape+(1,))

output1 = autoencoder.predict(img1_color)
output1 = output1*128

result = np.zeros((256, 256, 3))
result[:,:,0] = img1_color[0][:,:,0]
result[:,:,1:] = output1[0]

plt.imshow(lab2rgb(result))
# imsave("result.jpg", lab2rgb(result))