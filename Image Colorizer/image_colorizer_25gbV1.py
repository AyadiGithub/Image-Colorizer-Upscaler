# -*- coding: utf-8 -*-
"""Image Colorizer 25GB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HClc7Nx5ygE0TpbMSDfHbBN9O_Fi07u3
"""

# In[1]: Imports
import os
import re
from skimage.transform import resize, rescale
from skimage.color import rgb2lab, lab2rgb
from skimage.io import imsave
import matplotlib.pyplot as plt
import numpy as np
np.random.seed(0)
import tensorflow as tf
print(tf.__version__)
tf.test.gpu_device_name()

import tensorflow.keras as keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.python.client import device_lib
from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, BatchNormalization
from tensorflow.keras.layers import Conv2DTranspose, UpSampling2D, add
from tensorflow.keras.models import Model
from tensorflow.keras import regularizers
from tensorflow.keras import backend
device_lib.list_local_devices()

# Download and Unzip File
!wget https://uc73f117b990afa49c7f22236b84.dl.dropboxusercontent.com/cd/0/get/A7IPh8yN5_zbaLRNRLfb1hl_hGiSpD7aN1U6qMXny4xWNr8kckG4EQcPdR_wv4oAVXCmAzbHhp2PHPldPyKb2ze1crswby9fA4d8ifZyGrHjLTMm_7P-IYMt8ty0CKqFURw/file?dl=1

!unzip '/content/Ali_Foreman_dataset'

# Lets design the Encoder for the Autoencoder
# We will use the Functional API

# Input layer will be 256x256x3 (3 RGB channels as image depth)
input_img = Input(shape = (256, 256, 1)) # Using Input class from Keras

# Activity regularizer will regularize the output after activation.
# 64 3by3 filters with same padding
layer1 = Conv2D(64, (3, 3), padding = 'same',
                activation = 'elu', kernel_initializer='he_normal')(input_img)  # applied to the input image

layer2 = BatchNormalization()(layer1)                

layer3 = Conv2D(128, (3, 3), strides=(2, 2), padding = 'same',
                activation = 'elu', kernel_initializer='he_normal')(layer2)  # applied to the output of layer1

layer4 = BatchNormalization()(layer3)
# To learn new features in the smaller 'space',
# We need a bigger convolutional layer to make up for losing information due to the smaller space
layer5 = Conv2D(128, (3, 3), padding = 'same',
                activation = 'elu', kernel_initializer='he_normal')(layer4)  # applied to the output of layer3

layer6 = BatchNormalization()(layer5)                 

layer7 = Conv2D(256, (3, 3), strides=(2, 2),  padding = 'same',
                activation = 'elu', kernel_initializer='he_normal')(layer6)  # applied to the output of layer4

layer8 = BatchNormalization()(layer7)

# The depth of the encoder can be adjusted but so far it is deep enough relevant to the input
# Last encoder layer with more filters to make up for the loss of information
layer9 = Conv2D(256, (3, 3), padding = 'same',
                activation = 'elu', kernel_initializer='he_normal')(layer8) #applied to the output of layer6

# We will use the Model class from keras to put the model together
encoder = Model(input_img, layer9)

# Lets see the encoder model summary
encoder.summary()

# Now we need to do the reverse of Encoder for the decoder
layer10 = BatchNormalization()(layer9)

layer11 = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same',
               activation ='elu', kernel_initializer ='he_normal',
               activity_regularizer = regularizers.l1(10e-10))(layer10)

layer12 = BatchNormalization()(layer11)

layer13 = Conv2D(128, (3, 3), padding ='same',
               activation = 'elu', kernel_initializer='he_normal',
               activity_regularizer = regularizers.l1(10e-10))(layer12)

# We create a merge layer between layer5 and layer10 to add their inputs
# Merging them helps to prevent the vanishing gradient by sharing information(such as weights)
layer14 = add([layer5, layer13])  # Merge layer between layer 8 and 15

layer15 = BatchNormalization()(layer14)

layer16 = Conv2DTranspose(128, (3, 3), strides = (2, 2),  padding = 'same',
               activation = 'elu', kernel_initializer='he_normal',
               activity_regularizer = regularizers.l1(10e-10))(layer15)

layer17 = BatchNormalization()(layer16)

layer18 = Conv2D(64, (3, 3), padding = 'same',
               activation = 'elu', kernel_initializer='he_normal',
                activity_regularizer = regularizers.l1(10e-10))(layer17)

layer19 = add([layer18, layer1])

layer20 = BatchNormalization()(layer19)

layer21 = Conv2D(32, (3, 3), padding = 'same',
               activation = 'elu', kernel_initializer='he_normal',
               activity_regularizer = regularizers.l1(10e-10))(layer20)

layer22 = BatchNormalization()(layer21)               

# Lets create the decoder output. We need 3 neurons for 3 RGB channels
decoder = Conv2D(2, (3, 3), padding = 'same',
               activation = 'tanh', kernel_initializer='glorot_normal',
               activity_regularizer = regularizers.l1(10e-10))(layer22)

# Lets combine the encoder and decoder to an Autoencoder using Model class
autoencoder = Model(input_img, decoder)

# Lets create a summary
autoencoder.summary()

# Loss function DSSIMLOSS

def dssimloss(y_true, y_pred):
    ssim = tf.image.ssim(y_true, y_pred, 1.0)

    return backend.mean((1 - ssim)/2)

# Define optimizer and compile Model Autoencoder
# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)

autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

# Checkpoint to save best weights

checkpoint_filepath = '/content/checkpoints/'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

# Reducing learning rate if no decrease in Val_loss 
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,
                              patience=20, min_lr=1e-6)

# Train function


def train_batches(just_load_dataset=False):

    # Batch size
    batches = 256

    # point in current batch
    batch = 0

    # Current batch number
    batch_nb = 0

    max_batches = -1 #No limit to number of batches

    # number of epochs
    epochs = 50

    x_train = []
    x_train_down = []

    x_train_1 = []  
    x_train_down1 = []  

    data_set_path = '/content/AlivsForeman'

    for dirpath, dirnames, filenames in os.walk(data_set_path):

        for filename in filenames:
            if re.search("\.(jpg|jpeg|JPEG|png)$", filename):
                if batch_nb == max_batches:
                    return x_train_1, x_train_down1

                filepath = os.path.join(dirpath, filename)  # set path for each image

                image = plt.imread(filepath)  # read image from path
                
                image_resized = resize(image, (256, 256))  # resizing images
                lab = rgb2lab(image_resized)
                x_train_down.append(lab[:,:,0])
                x_train.append(lab[:,:,1:] / 128)
                batch += 1

                if batch == batches:

                    batch_nb += 1

                    x_train_1 = np.array(x_train)
                    x_train_down1 = np.array(x_train_down)
                    x_train_down1 = x_train_down1.reshape(x_train_down1.shape+(1,))

                    if just_load_dataset:  # If just_load_dataset is set to True
                        return x_train_1 , x_train_down1  # return arrays

                    print('Training batch', batch_nb, '(', batches, ')')

                    autoencoder.fit(x_train_down1, x_train_1, epochs=epochs, batch_size=16, shuffle=True, validation_split=0.20, callbacks=[model_checkpoint_callback])

                    x_train = []
                    x_train_down = []

                    batch = 0

    return x_train_1, x_train_down1

x_train, x_train_down = train_batches()

autoencoder.save('/content/Colorizer.h5')

# Saving model to json
autoencoder_json = autoencoder.to_json()
with open("model.json", "w") as json_file:  # Store model.json
    json_file.write(autoencoder_json)

autoencoder.save_weights('/content/autoencoder_weightsv2.h5')
encoder.save_weights('/content/encoder_weightsv2.h5')

x_train, x_train_down = train_batches(just_load_dataset=True)

# Load autoencoder pre-trained weights from trained model
autoencoder.load_weights('/content/autoencoder_weightsv2.h5')

# Load encoder weights
encoder.load_weights('/content/encoder_weightsv2.h5')

# Next, we will feed the encoder the input images without decoding them
encoded_imgs = encoder.predict(x_train_down)

# Shape of encoded images arrays
encoded_imgs.shape

# We use numpy.clip to prevent the new image from having 'off' colors
# clip as max = 1.0 and min = 0.0
upscaled_img = np.clip(autoencoder.predict(x_train_down), 0.0, 1.0)  # Prediction made on low-res image

# Image index from 0 to 64 (batch size)
image_index = 15

autoencoder.save('/content/MyNetwork.h5')
model = tf.keras.models.load_model('/content/MyNetwork.h5',
                                   custom_objects=None,
                                   compile=True)

img1_color=[]

img1 = img_to_array(load_img('/content/Muhammad-Ali-George-Foreman.jpg'))
img1 = resize(img1 ,(256,256))
img1_color.append(img1)

img1_color = np.array(img1_color, dtype=float)
img1_color = rgb2lab(1.0/255*img1_color)[:,:,:,0]
img1_color = img1_color.reshape(img1_color.shape+(1,))

output1 = autoencoder.predict(img1_color)
output1 = output1*128

result = np.zeros((256, 256, 3))
result[:,:,0] = img1_color[0][:,:,0]
result[:,:,1:] = output1[0]
plt.imshow(lab2rgb(result))
# imsave("result.png", lab2rgb(result))

autoencoder.save('/content/MyNetwork.h5')

# Saving model to json
autoencoder_json = autoencoder.to_json()
with open("model.json", "w") as json_file:  # Store model.json
    json_file.write(autoencoder_json)

autoencoder.save_weights('/content/autoencoder_weights.h5')
encoder.save_weights('/content/encoder_weights.h5')

encoded_imgs.shape

# Lets plot the images and compare:
# Original low-res vs autoencoded upscale vs original
plt.figure(figsize=(256, 256))
i = 1
ax = plt.subplot(10, 10, i)
plt.imshow(x_train_down[image_index])  # Show low-res image

i += 1
ax = plt.subplot(10, 10, i)
plt.imshow(encoded_imgs[image_index].reshape(256, 256, 1))

i += 1
ax = plt.subplot(10, 10, i)
plt.imshow(upscaled_img[image_index])  # Show upscaled image

i += 1
ax = plt.subplot(10, 10, i)
plt.imshow(x_train[image_index]) # Show original high-res image

plt.show()

# It can be see that the model prediction has artifacts due to lack of training
# Model needs more training but produced a sharper image than the Original