# -*- coding: utf-8 -*-
"""Image Upscaler 25GB V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IOY7WXs7RqxFPd3ADlotqWmj1akiISaP
"""

# In[1]: Imports

import os
import re
from skimage.transform import resize, rescale
from matplotlib import pyplot
import matplotlib.pyplot as plt
import numpy as np
from multiprocessing import Pool
np.random.seed(0)
import pydot
import graphviz
import tensorflow as tf
from keras.utils.vis_utils import plot_model
from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, BatchNormalization
from tensorflow.keras.layers import Conv2DTranspose, UpSampling2D, add
from tensorflow.keras.models import Model
from tensorflow.keras import regularizers
from tensorflow.keras import backend
tf.test.gpu_device_name()
from tensorflow.python.client import device_lib
device_lib.list_local_devices()
print(tf.__version__)

# In[2]: Encoder
# Encoder for the Autoencoder
# Using the Functional API

# Input layer will be 256x256x3 (3 RGB channels as image depth)
input_img = Input(shape=(256, 256, 3))  # Using Input class from Keras

# Activity regularizer will regularize the output after activation.
layer1 = Conv2D(32, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(input_img)  # Applied to the input image

layer2 = BatchNormalization()(layer1)

layer3 = Conv2D(64, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(layer2)  # Applied to the output of layer1

# Downscaling picture dimensions with scale factor of 2 using MaxPool2D
layer4 = MaxPooling2D(padding='same')(layer3)  # Applied to layer 2 output

layer5 = BatchNormalization()(layer4)
# To learn new features in the smaller 'space',
# We need a bigger convolutional layer to make up for losing information due to the smaller space
layer6 = Conv2D(128, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(layer5)  # Applied to the output of layer3

layer7 = BatchNormalization()(layer6)

layer8 = Conv2D(128, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(layer7)  # Applied to the output of layer4

# Downscaling picture dimensions with scale factor of 2 using MaxPool2D
layer9 = MaxPooling2D(padding='same')(layer8)  # Applied to layer 2 output

layer10 = BatchNormalization()(layer9)

# The depth of the encoder can be adjusted but so far it is deep enough relevant to the input
# Last encoder layer with more filters to make up for the loss of information
layer11 = Conv2D(256, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(layer10)  # Applied to the output of layer6

# We will use the Model class from keras to put the model together
encoder = Model(input_img, layer11)

# Lets see the encoder model summary
encoder.summary()

# In[3] Decoder

# Reverse of the Encoder for the Decoder
layer12 = BatchNormalization()(layer11)

layer13 = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same',
               activation ='elu', kernel_initializer ='he_normal',
               activity_regularizer = regularizers.l1(10e-10))(layer12)

layer14 = BatchNormalization()(layer13)

layer15 = Conv2D(128, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(layer14)

# We create a merge layer between layer5 and layer10 to add their inputs
# Merging them helps to prevent the vanishing gradient by sharing information(such as weights)
layer16 = add([layer8, layer15])  # Merge layer between layer 8 and 15

layer17 = BatchNormalization()(layer16)

layer18 = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same',
               activation='elu', kernel_initializer='he_normal',
               activity_regularizer=regularizers.l1(10e-10))(layer17)

layer19 = BatchNormalization()(layer18)

layer20 = Conv2D(64, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(layer19)

layer21 = add([layer20, layer3])

layer22 = BatchNormalization()(layer21)

layer23 = Conv2D(32, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(layer22)

layer24 = BatchNormalization()(layer23)

# Lets create the decoder output. We need 3 neurons for 3 RGB channels
decoder = Conv2D(3, (3, 3), padding='same',
                activation='elu', kernel_initializer='he_normal',
                activity_regularizer=regularizers.l1(10e-10))(layer24)

# Lets combine the encoder and decoder to an Autoencoder using Model class
autoencoder = Model(input_img, decoder)

# Lets create a summary
autoencoder.summary()

# In[4]

# Loss function DSSIMLOSS


def dssimloss(y_true, y_pred):
    ssim = tf.image.ssim(y_true, y_pred, 2.0)

    return backend.mean((1 - ssim)/2)


# Define optimizer and compile Model Autoencoder
autoencoder.compile(optimizer='Adam', loss=dssimloss)

# Import google drive to colab
from google.colab import drive
drive.mount('/content/drive')

# Download and Unzip File
!unzip '/content/drive/My Drive/dataset.zip' -d dataset

# Checkpoint to save best weights
checkpoint_filepath = '/content/checkpoints/'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

# Reducing learning rate if no decrease in Val_loss. Non-Adam optimizers
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',
                                                 factor=0.1,
                                                 patience=10, min_lr=1e-7)

# In[5] Training


def train_batches(just_load_dataset=False):

    # Training on Tesla P-100 (16GB VRAM)
    # Batch size
    batches = 128

    # point in current batch
    batch = 0

    # Current batch number
    batch_nb = 0

    max_batches = -1  # No limit to number of batches

    # number of epochs
    epochs = 50

    x_train = []
    x_train_down = []

    x_train_1 = []  # high-res image array for x_train
    x_train_down1 = []  # array for low-res copy x_train_down

    data_set_path = '/content/dataset'

    for dirpath, dirnames, filenames in os.walk(data_set_path):

        for filename in filenames:
            if re.search("\.(jpg|jpeg|JPEG|png)$", filename):
                if batch_nb == max_batches:
                    return x_train_1, x_train_down1

                filepath = os.path.join(dirpath, filename)  # Set path for each image

                image = pyplot.imread(filepath)  # Read image from path

                if len(image.shape) > 2:

                    # Resizing images that are larger than 256,256 to be 256,256
                    image_resized = resize(image, (256, 256))  # Resizing images

                    x_train.append(image_resized)  # Adding resized image to x_train

                    x_train_down.append(rescale(rescale(image_resized, 0.5, multichannel = True), 2.0, multichannel = True))  # Downsampling resized image to low-res

                    batch += 1

                    if batch == batches:

                        batch_nb += 1

                        x_train_1 = np.array(x_train)
                        x_train_down1 = np.array(x_train_down)

                        if just_load_dataset:  # If just_load_dataset is set to True
                            return x_train_1 , x_train_down1  # return arrays

                        print('Training batch', batch_nb, '(', batches, ')')

                        autoencoder.fit(x_train_down1, x_train_1, epochs=epochs, batch_size=64, shuffle=True, validation_split=0.20, callbacks=[model_checkpoint_callback])

                        x_train = []
                        x_train_down = []

                        batch = 0

    return x_train_1, x_train_down1


# Start Training
x_train, x_train_down = train_batches()

# Save keras model
autoencoder.save('/content/MyNetworkv2.h5')

# Saving model to json
autoencoder_json = autoencoder.to_json()
with open("model.json", "w") as json_file:  # Store model.json
    json_file.write(autoencoder_json)

autoencoder.save_weights('/content/autoencoder_weightsv2.h5')
encoder.save_weights('/content/encoder_weightsv2.h5')

x_train, x_train_down = train_batches(just_load_dataset=True)

# Load autoencoder pre-trained weights from trained model
autoencoder.load_weights('/content/autoencoder_weightsv2.h5')

# Load encoder weights
encoder.load_weights('/content/encoder_weightsv2.h5')

# Next, we will feed the encoder the input images without decoding them
encoded_imgs = encoder.predict(x_train_down)

# Shape of encoded images arrays
encoded_imgs.shape

# We use numpy.clip to prevent the new image from having 'off' colors
# clip as max = 1.0 and min = 0.0
upscaled_img = np.clip(autoencoder.predict(x_train_down), 0.0, 1.0)  # Prediction made on low-res image

# Image index from 0 to 64 (batch size)
image_index = 60

# Lets plot the images and compare:
# Original low-res vs autoencoded upscale vs original
plt.figure(figsize=(256, 256))
i = 1
ax = plt.subplot(10, 10, i)
plt.imshow(x_train_down[image_index])  # Show low-res image

i += 1
ax = plt.subplot(10, 10, i)
plt.imshow(encoded_imgs[image_index].reshape(64*64, 256))

i += 1
ax = plt.subplot(10, 10, i)
plt.imshow(upscaled_img[image_index])  # Show upscaled image

i += 1
ax = plt.subplot(10, 10, i)
plt.imshow(x_train[image_index]) # Show original high-res image

plt.show()
# Model needs more training but produced a much better image than the Original.


